{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/header.png\"></center>\n",
    "\n",
    "<h1><center>Лекция №4: Введение в машинное обучение</center></h1>\n",
    "<hr>\n",
    "<h2><center>Методы обучения без учителя: Кластеризация</center></h2>\n",
    "<h3><center>Шестаков Андрей</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Методы обучение без учителя (Unsupervised)\n",
    "\n",
    "* В чем отличие от Supervised методов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Кластеризация\n",
    "* Уменьшение размерности\n",
    "    * Метод главных компонент\n",
    "    * Многомерное шкалирование\n",
    "    * Тематические модели*\n",
    "* Поиск ассоциативных правил"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Кластеризация\n",
    "\n",
    "Основная задача кластерного анализа — разбиение исходного набора объектов на группы (кластеры) таким образом, чтобы объекты в группе были похожи друг на друга, а объекты из разных групп - отличались.\n",
    "\n",
    "<center><img src=\"https://i.ytimg.com/vi/zPJtDohab-g/maxresdefault.jpg\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Цели кластерного анализа\n",
    "\n",
    "* **Поиск структуры** в данных и ее **интерпретация**\n",
    "* Поиск аномальных объектов\n",
    "* Детальный анализ отдельных кластеров\n",
    "* Формирование признаков на основе кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Группы методов\n",
    "\n",
    "* Методы основанные на прототипах\n",
    "    * Каждый кластер ассоциируется с виртуальрным \"эталонным\" объектом\n",
    "* Иерархические методы\n",
    "    * Не простое разбиение на целая иерархия\n",
    "* Плотностные методы\n",
    "    * Ищем плотные скопления точек в признаковом пространстве\n",
    "* Вероятностные методы\n",
    "    * Предполагаем, что данные порождены некоторой смесью вероятностных распределений\n",
    "* Спектральные методы\n",
    "    * Испольюзуем замечательные спектральные свойства разных матриц\n",
    "* Сеточные методы\n",
    "    * Бьем признаковое пространство на сегменты\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм k-means\n",
    "### Неформальное [демо](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Алгоритм k-means\n",
    "\n",
    "* Дано множество объектов $X = \\{x_1, x_2, \\dots, x_N\\}$\n",
    "* Кластер $C_k \\Leftrightarrow \\text{ центройд } \\mu_k$\n",
    "* Объект $x_i \\in C_k \\Leftrightarrow \\mu_k = \\arg \\min\\limits_{\\mu_j} \\|x_i - \\mu_j \\|^2$\n",
    "* Надо найти такое разбиение на $K$ кластеров, чтобы минизировать\n",
    "$$ L(C) = \\sum_{k=1}^K\\sum_{i\\in C_k} ||x_i - \\mu_k||^2 \\rightarrow \\min\\limits_C $$\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum _{x_n \\in C_k} x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Метод $k$-средних является итеративным алгоритмом разбиения множества объектов на $K$ кластеров "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Алгоритм k-means\n",
    "1. Выбрать $K$ начальных центроидов случайным образом  $\\rightarrow \\mu_k, \\ k=1\\dots K$\n",
    "2. Для каждой точки из датасета присвоить кластер, соответствующий ближайшему центроиду\n",
    "$$C_k = \\{x_n : ||x_n - \\mu_k||^2 \\leq ||x_n - \\mu_l||^2 \\quad \\forall l \\neq k \\} $$\n",
    "3. Обновить центройды: \n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum _{x_n \\in C_k} x_n$$\n",
    "4. Повторять 2 и 3 до тех пор, пока изменения перестанут быть существенными \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Kmeans_animation.gif' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Основные факторы\n",
    "* Начальная инициализация центройдов\n",
    "* Количество кластеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kак выбрать K?\n",
    "\n",
    "* Не пользоваться обычным k-means (X-means, ik-means)\n",
    "* Посмотреть на меры качества кластеризации\n",
    "* Воспользоваться эвристиками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elbow method (Метод локтя)\n",
    "\n",
    "* Критерий минимизации k-means\n",
    "$$ L(C) = \\sum_{k=1}^K\\sum_{i\\in C_k} ||x_i - \\mu_k||^2 \\rightarrow \\min\\limits_C $$\n",
    "* Давайте возьмем всевозможные $K$, для каждого запустим алгоритм, посчитаем на результате $L(C)$ и выберем минимум!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ничего не выйдет... Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     20,
     34,
     40
    ],
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "\n",
    "\n",
    "crit = []\n",
    "\n",
    "for k in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    crit.append(np.sqrt(kmeans.inertia_))\n",
    "    \n",
    "def elbow_demo(k=2):\n",
    "    \n",
    "    X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    ax[0].scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
    "    \n",
    "    ax[0].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                  marker='o', c=\"white\", alpha=1, s=200)\n",
    "    \n",
    "    ax[0].set_xlabel('$x_1$')\n",
    "    ax[0].set_ylabel('$x_2$')\n",
    "\n",
    "    for i, c in enumerate(kmeans.cluster_centers_):\n",
    "        ax[0].scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "        \n",
    "    ax[1].plot(range(2,8), crit, marker='s')\n",
    "    \n",
    "    ax[1].set_xlabel('$k$')\n",
    "    ax[1].set_ylabel('$L^{(k)}(C)$')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elbow method (Метод локтя)\n",
    "\n",
    "* Выбирают такое $k$, после которого функционал $L(C)$ уменьшается не слишком быстро\n",
    "* Чуть более формально:\n",
    "$$ D(k) = \\frac{|L^{(k)}(C) - L^{(k+1)}(C)|}{|L^{(k-1)}(C) - L^{(k)}(C)|} \\quad \\text{\"невелико\"} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot = interact(elbow_demo, k=IntSlider(min=2,max=8,step=1,value=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Важно!\n",
    "* Эвристика и меры качества клатеризации носят лишь рекомендательный характер!\n",
    "* Если они ничего не дают, то лучше ориентироваться на свои знания в предметной области\n",
    "* Или \"выжать\" из полученной кластеризации максимум\n",
    "    * *3 из 5 полученных кластеров интерпретируются - и то хорошо*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Начальная инициализация центройдов\n",
    "* Выбрать координаты $K$ случайных объектов из датасета\n",
    "    * Производить случайные запуски много раз и выбрать наиболее оптимальную инициализацию\n",
    "* Использовать результат другой кластеризации на $K$ кластеров\n",
    "* k-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-means++\n",
    "* Первый центройд выбираем случайным образом из объектов датасета\n",
    "* Для каждой точки рассчитываем расстояние $d_{\\min}(x_i) = \\min_{\\mu_j} \\|x_i - \\mu_j\\|^2$\n",
    "* Точка назначается следующим центройдом с вероятностью $p(x_i) \\propto d_{\\min}(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def demo_kmpp(iters=1):\n",
    "\n",
    "    X, y = make_blobs(n_samples=550, cluster_std=1.5, n_features=2, centers=5, random_state=12345)\n",
    "\n",
    "    X_grid1, X_grid2 = np.meshgrid(np.linspace(-12, 18, 500),\n",
    "                                   np.linspace(-11, 8, 500))\n",
    "\n",
    "    XX = np.c_[X_grid1.flatten(), X_grid2.flatten()]\n",
    "    np.random.seed(1)\n",
    "    centroids = np.empty((0, 2))\n",
    "\n",
    "    for i in range(iters):\n",
    "        if i == 0:\n",
    "            d = np.ones_like(y, dtype=float)\n",
    "        else:\n",
    "            d = pairwise_distances(X, centroids, metric='euclidean').min(axis=1)\n",
    "        weights = d/d.sum()\n",
    "\n",
    "        centroid_idx = np.random.choice(X.shape[0], size=1, replace=False, p=weights)[0]\n",
    "        centroids = np.r_[centroids, X[centroid_idx, np.newaxis]]\n",
    "\n",
    "    d_grid = pairwise_distances(XX, centroids, metric='euclidean').min(axis=1)\n",
    "\n",
    "    d_grid = d_grid.reshape(X_grid1.shape)\n",
    "    d_grid = d_grid/d_grid.max()\n",
    "\n",
    "    levels = np.linspace(0, 1, 100)\n",
    "\n",
    "    plt.contourf(X_grid1, X_grid2, d_grid, cmap=plt.cm.Blues, alpha=0.7, levels=levels)\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=100)\n",
    "\n",
    "    centers = centroids\n",
    "    \n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=500, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        plt.scatter(c[0], c[1], marker='$%d$' % (i+1), alpha=1,\n",
    "                    s=100, edgecolor='k')\n",
    "\n",
    "    plt.xlabel('$x_1$', fontsize=15)\n",
    "    plt.ylabel('$x_2$', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(demo_kmpp, iters=IntSlider(min=1,max=6,step=1,value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "* Метод k-средних – жадный итеративный алгоритм\n",
    "* Зависит от начальных центройдов и их количества\n",
    "\n",
    "#### Преимущества\n",
    "* Прост как пробка\n",
    "* Имеет множество модификаций\n",
    "* Интерпретация кластеров через центройды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Недостатки\n",
    "\n",
    "* Подразумевает выпуклые кластеры\n",
    "<center><img src='images/kmeans_2moons.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Всегда* на выходе будет k кластеров\n",
    "<center><img src='images/kmeans_digits.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='images/kmeans-guys.jpeg'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Спектральная кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Небольшой спойлер к лекцям по сетям\n",
    "\n",
    "**Граф** - математическая модель представления сетевых структур.\n",
    "\n",
    "Графом $\\mathcal{G}$ называется пара множеств $\\mathcal{G} = (V, E)$, где $V$ содержит в себе множество *вершин (узлов, vertices, nodes)*, а $E$ содержит множество *ребер (связей, edges, links)*. <br/> \n",
    "\n",
    "<center><img src='images/clique_init.png' width=800></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Матрица смежности, Incidence matrix, Association matrix.\n",
    "\n",
    "Матрица смежности - это квадратная матрица размера $n \\times n$. Обозначим ее буквой $\\mathbf{A}$ <br/>\n",
    "\n",
    "**В невзвешенном графе:**<br/>\n",
    "$\\mathbf{A_{ij}} = 1$, если в графе есть ребро между вершинами $v_i$ и $v_j$ <br/>\n",
    "$\\mathbf{A_{ij}} = 0$, иначе\n",
    "\n",
    "**Во взвешенном графе:**<br/>\n",
    "$\\mathbf{A_{ij}} = w_{ij}$, если в графе есть ребро между вершинами $v_i$ и $v_j$ <br/>\n",
    "$\\mathbf{A_{ij}} = 0$, иначе\n",
    "\n",
    "Для графа выше матрица смежности имеет следующий вид:\n",
    "\n",
    "<center><img src='images/clique_matrix.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = \\\n",
    "np.array([[0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n",
    "          [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 1],\n",
    "          [0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "          [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],\n",
    "          [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "          [0, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n",
    "\n",
    "d = A.sum(axis=1) # Степени вершин (количество соседей)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "D = np.diag(d)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Компоненты связности и Лапласиан\n",
    "* На время представим, что в графе отсутствуют ребра $e_{3,7}$, $e_{2,8}$, $e_{6,10}$\n",
    "<center><img src='images/clique_init.png' width=800></center>\n",
    "* Тогда в графе возникают 2 связные компоненты $\\{v_1,v_2,v_3,v_4,v_5,v_6\\}$ и $\\{v_7,v_8,v_9,v_{10}\\}$\n",
    "* Рассмотрим матрицу $L = D-A$ - Лапласиан"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = \\\n",
    "np.array([[0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "          [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "          [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n",
    "          [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "          [0, 0, 0, 0, 0, 0, 0, 1, 1, 0]])\n",
    "d = A.sum(axis=1) \n",
    "D = np.diag(d)\n",
    "L = D-A\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Замечательные свойства Лапласиана\n",
    "\n",
    "* Симметричная и положительно определенная матрица\n",
    "$$ \\forall f: f^\\top L f = f^\\top D f -  f^\\top A f = \\frac{1}{2} \\left(\\sum_{i=1}^nd_i f^2_i - 2\\sum_{i,j=1}^nf_if_ja_{ij} + \\sum_{j=1}^nd_j f^2_j\\right) = \\frac{1}{2} a_{ij}(f_i-f_j)^2 \\geq 0$$\n",
    "* Все собственные числа $\\lambda_1\\leq\\dots\\leq\\lambda_n$ неотрицательные \n",
    "* Наименьшее собственное число $\\lambda_1=0$ с единичным собственныем вектором\n",
    "* Если в графе с матрицей смежности $A$ есть $k$ компонент, то кратность собственного числа $\\lambda_1=0$ равна $k$, а соответствующие собственные вектора задают разбиение на компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "eig_vals, eig_vects = np.linalg.eig(L)\n",
    "idx = np.argsort(eig_vals) # отсортируем по возрастанию\n",
    "eig_vects = eig_vects[:,idx] # так же пересортируем\n",
    "\n",
    "print(eig_vects[:,0])\n",
    "print(eig_vects[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Вернемся в неблагоприятный кейс:\n",
    "# вернем недостающие ребра и повторим расчеты\n",
    "A = \\\n",
    "np.array([[0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n",
    "          [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
    "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 1],\n",
    "          [0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "          [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],\n",
    "          [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "          [0, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n",
    "d = A.sum(axis=1) \n",
    "D = np.diag(d)\n",
    "L = D-A\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "eig_vals, eig_vects = np.linalg.eig(L)\n",
    "idx = np.argsort(eig_vals) # отсортируем по возрастанию\n",
    "eig_vects = eig_vects[:,idx] # так же пересортируем\n",
    "\n",
    "Y = eig_vects[:,:2] # Возьмем 2 наименьших собственных вектора j\n",
    "# 2 - потому что хотим разделить на 2 части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "А это уже можно кластеризовать с помощью k-means и получить разбиение графа!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Спектральная кластеризация для табличных данных\n",
    "\n",
    "* Имея на входе исходные данные (объекты по строкам, признаки по столбцам) надо получить что-то наподобии матрицы смежности $A$\n",
    "* Если объекты $x_i$ и $x_j$ \"похожи\", то $a_{ij} \\rightarrow 1$\n",
    "* Можно например использовать функцию \n",
    "$$A_{ij} = K(x_i,x_j) = \\exp\\left(\\frac{-\\|x_i-x_j\\|^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aлгоритм\n",
    "**Вход:**\n",
    "* Данные $X\\in\\mathbb{R}^{n\\times m}$\n",
    "* Kоличество кластеров $k$\n",
    "* Ширина окна $\\sigma$\n",
    "\n",
    "**Шаги**\n",
    "1. Расчитываем афинитивность между всеми парами объектов - получаем матрицу $A$ $$A_{ij} = \\exp\\left(\\frac{-\\|x_i-x_j\\|^2}{2\\sigma^2}\\right)$$\n",
    "2. Считаем лапласиан $L=D-A$\n",
    "3. Находим $k$ наименьших собственных векторов матрицы $L$ - Формируем матрицу $Y\\in \\mathbb{R}^{n\\times k}$\n",
    "4. Кластеризуем на $k$ кластеров с помощью метода $k$-средних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(random_state=123, noise=0.1)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "# model = KMeans(n_clusters=2)\n",
    "model = SpectralClustering(n_clusters=2, gamma=30, random_state=123)\n",
    "\n",
    "model.fit(X)\n",
    "labels = model.labels_\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Оценка качества кластеризаци"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Оценка качества кластеризации при известном groud truth\n",
    "\n",
    "Пусть $\\hat{\\pi}$ - это полученное разбиение на кластеры, а $\\pi^*$ - ground truth. \n",
    "\n",
    "<center><img src='http://cse3521.artifice.cc/images/iris-true-class-sepal.png' width=800></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Adjusted Rand Index\n",
    "\n",
    "$$ \\text{Rand}(\\hat{\\pi},\\pi^*) = \\frac{a + d}{a + b + c + d} \\text{,}$$\n",
    "где \n",
    "* $a$ количество пар объектов, находящихся в одинаковых кластерах в $\\hat{\\pi}$ и\n",
    "$\\pi^*$, \n",
    "* $b$ ($c$) количество пар объектов в одном и том же кластере в  $\\hat{\\pi}$ ($\\pi^*$), но в разных в  $\\pi^*$ ($\\hat{\\pi}$)\n",
    "* $d$ количество пар объектов в разных кластерах в $\\hat{\\pi}$ и $\\pi^*$\n",
    "\n",
    "<center><img src='images/rand1.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Индекс Жаккара\n",
    "$$ Jac(\\hat{\\pi}, \\pi^*) = \\frac{a}{a + b + c}$$\n",
    "\n",
    "#### Точность, полнота и F-мера\n",
    "$$ Precision(\\hat{\\pi}, \\pi^*) = \\frac{a}{a + b} $$\n",
    "$$ Recall(\\hat{\\pi}, \\pi^*) = \\frac{a}{a + c} $$\n",
    "$$ F-measure(\\hat{\\pi}, \\pi^*) = \\frac{2Precision \\cdot Recall}{Precision + Recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Меры валидности кластеров (no ground truth)\n",
    "\n",
    "* Измеряют полученое разбиения по отношению к качествам хорошей кластеризации\n",
    "    * Компактность объектов внутри кластера\n",
    "    * Разделимость кластеров друг от друга\n",
    "    \n",
    "Про различные меры качества кластризации и меры валидности кластеров в sklearn можно почитать [тут](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Критерий Silhouette\n",
    "\n",
    "Пусть дана кластеризация в $K$ кластеров, и объект $i$ попал в $C_k$\n",
    "\n",
    "* $a(i)$ -- среднее расстояние от $i$ объекта до объектов из $C_k$\n",
    "* $b(i) = min_{j \\neq k} b_j(i)$,  где $b_j(i)$ -- среднее расстояние от $i$ объекта до объектов из $C_j$\n",
    "$$\n",
    "silhouette(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "Средний silhouette для всех точек из $\\mathbf{X}$ является критерием качества кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/sil1.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/sil2.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Важно!\n",
    "* Эвристики и меры качества клатеризации носят лишь рекомендательный характер!\n",
    "* Если они ничего не дают, то лучше ориентироваться на свои знания в предметной области\n",
    "* Или \"выжать\" из полученной кластеризации максимум\n",
    "    * *3 из 5 полученных кластеров интерпретируются - и то хорошо*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Литература\n",
    "* [Mohammed J. Zaki, Ch3](https://www.amazon.com/Data-Mining-Analysis-Fundamental-Algorithms/dp/0521766338)\n",
    "* [Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman, Ch7](http://www.mmds.org/)\n",
    "* [Andrew R. Webb, Keith D. Copsey, Ch11](http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470682272.html)\n",
    "* [Tutorial on spectral clustering](https://arxiv.org/pdf/0711.0189.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вопросы?\n",
    "\n",
    "### Пожалуйста, напишите отзыв о лекции"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

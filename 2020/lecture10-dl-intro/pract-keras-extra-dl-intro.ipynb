{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/header.png\"></center>\n",
    "\n",
    "<h2><center>Лекция 10: Нейронные сети. Основы. (Тыкаем в керас 3)</center></h2>\n",
    "<h3><center>Шестаков Андрей</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "import seaborn as sn\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import regularizers as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you are using colab\n",
    "# !mkdir ./data\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/shestakoff/sphere-ml-intro/master/2020/lecture10-dl-intro/data/bikesharing.csv -O ./data/bikesharing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим датасет для оценки спроса на прокатные велосипеды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bikesharing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* dteday - date\n",
    "* yr - year code (2011 - 0, 2012 - 1)\n",
    "* month - month code\n",
    "* hr - hour code\n",
    "* holiday - whether the day is considered a holiday\n",
    "* weekday - weekday\n",
    "* workingday - whether the day is neither a weekend nor holiday\n",
    "* weather -\n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "* temp - temperature in Celsius\n",
    "* humidity - relative humidity\n",
    "* windspeed - wind speed\n",
    "* cnt - number of total rentals (Dependent Variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(18, 15)\n",
    "sn.boxplot(data=df, y=\"cnt\", orient=\"v\", ax=axes[0][0])\n",
    "sn.boxplot(data=df, y=\"cnt\", x=\"mnth\", orient=\"v\", ax=axes[0][1])\n",
    "sn.boxplot(data=df, y=\"cnt\", x=\"hr\", orient=\"v\", ax=axes[1][0])\n",
    "sn.boxplot(data=df, y=\"cnt\", x=\"weekday\", orient=\"v\", ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Count', title=\"Box Plot On Count\")\n",
    "axes[0][1].set(xlabel='Month', ylabel='Count', title=\"Box Plot On Count Across Year\")\n",
    "axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count', title=\"Box Plot On Count Across Hour Of The Day\")\n",
    "axes[1][1].set(xlabel='Working Day', ylabel='Count', title=\"Box Plot On Count Across Weekdays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.regplot(x=\"temp\", y=\"cnt\", data=df, ax=ax1)\n",
    "sn.regplot(x=\"windspeed\", y=\"cnt\", data=df, ax=ax2)\n",
    "sn.regplot(x=\"hum\", y=\"cnt\", data=df, ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать пайплайн\n",
    "\n",
    "* Все числовые признаки мы прогоним через StandartScaler\n",
    "* Бинарные признаки оставим как есть\n",
    "* Категориальные признаки - OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ['yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\n",
    "               'weathersit', 'temp', 'hum', 'windspeed']].values\n",
    "y = np.log(df.loc[:, 'cnt'].values+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = ColumnTransformer(\n",
    "    [('one-hot', OneHotEncoder(), [1,2,4,6]),\n",
    "     ('continious', StandardScaler(), [7,8,9])], \n",
    "    remainder='passthrough', \n",
    "    sparse_threshold=0 ) # Всегда возвращать dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preproc = preproc.fit_transform(X_train)\n",
    "X_valid_preproc = preproc.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее описываем модель по слоям\n",
    "# В Sequential каждый слой идет на вход в следующий\n",
    "model = keras.Sequential([\n",
    "    L.Dense(50, input_shape=[X_train_preproc.shape[1]],\n",
    "            activation='???', \n",
    "            kernel_regularizer=reg.l2(0.01), name='hidden'),\n",
    "    L.Dense(1, activation='???', kernel_regularizer=reg.l2(0.01),\n",
    "            name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam() # Задаем оптимизатор\n",
    "\n",
    "# Компилируем модель (keras строит граф вычислений, модель иницилизируется)\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae', 'mse']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid_preproc) # Она уже может предсказывать, правда пока белеберду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем обучение, лог обучения будет записываться в переменную metrics\n",
    "metrics = \\\n",
    "model.fit(x=X_train_preproc, \n",
    "          y=y_train, \n",
    "          batch_size=512, \n",
    "          epochs=200, \n",
    "          validation_data=[X_valid_preproc, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics.history['loss'][2:], label='train_mae')\n",
    "plt.plot(metrics.history['val_loss'][2:], label='val_mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.expm1(y_hat), np.expm1(y_valid))\n",
    "plt.plot(np.expm1(y_valid), np.expm1(y_valid), c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавим Dropout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    L.Dense(50, input_shape=[X_train_preproc.shape[1]],\n",
    "            activation='???',name='hidden'),\n",
    "    L.Dropout(0.2), \n",
    "    L.Dense(1, activation='???',\n",
    "            name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae', 'mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_valid_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = \\\n",
    "model.fit(x=X_train_preproc, \n",
    "          y=y_train, batch_size=512, \n",
    "          epochs=200, validation_data=[X_valid_preproc, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics.history['loss'][5:], label='train_mae')\n",
    "plt.plot(metrics.history['val_loss'][5:], label='val_mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим эмбеддингов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети предпочитают нормированные и плотные данные,а мы имеем разреженные вектора.\n",
    "\n",
    "Введем эмбединги для категориальных переменных, которые тоже будут обучаться вместе с остальными весами сети.\n",
    "\n",
    "Для этого придется немного похимичить и поработать с [Keras Functional API](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем получить что-то такое:\n",
    "<center><img src='./img/categ_emb.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \\\n",
    "np.array(['yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_variables = [1,2,4] # Колонки, для которых будут строится эмбединги\n",
    "asis_variables = [0,3,7,8,9] # Колонки, которые пойдут почти без изменений\n",
    "\n",
    "print(columns[categ_variables])\n",
    "print(columns[asis_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас мы разделим исходую матрицу `X` на кусочки:\n",
    "* по кусочку на каждую категориальную переменную\n",
    "* кусочек на все остельные переменные (бинарные + нормализованные числовые)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим нормализатор\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[:, [7,8,9]])\n",
    "\n",
    "X_train_asis = np.c_[X_train[:,[0,3]], scaler.transform(X_train[:, [7,8,9]])]\n",
    "X_train_month = X_train[:,[categ_variables[0]]]-1 # -1 чтобы значения были от 0 до n-1\n",
    "X_train_hour = X_train[:,[categ_variables[1]]]\n",
    "X_train_weekday = X_train[:,[categ_variables[2]]]\n",
    "\n",
    "X_valid_asis = np.c_[X_valid[:,[0,3]], scaler.transform(X_valid[:, [7,8,9]])]\n",
    "X_valid_month = X_valid[:, [categ_variables[0]]]-1  # -1 чтобы значения были от 0 до n-1\n",
    "X_valid_hour = X_valid[:, [categ_variables[1]]]\n",
    "X_valid_weekday = X_valid[:,[categ_variables[2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И теперь буквально тоже по кирпичикам описываем наши компоненты\n",
    "\n",
    "# Сначала входы нейронной сети:\n",
    "cont_input = L.Input(shape=(5,), name='cont_variables')\n",
    "month_input = L.Input(shape=(1,), dtype='int32')\n",
    "weekday_input = L.Input(shape=(1,), dtype='int32')\n",
    "hour_input = L.Input(shape=(1,), dtype='int32')\n",
    "\n",
    "# Эмбеддинги\n",
    "month_embedding = L.Embedding(input_dim=12, # Количество уникальных значений\n",
    "                              output_dim=4, # Длина эмбединга\n",
    "                              name='month_emb')(month_input) # На вход подаем input\n",
    "month_embedding = L.Flatten()(month_embedding) # Размазываем слой\n",
    "\n",
    "weekday_embedding = ...\n",
    "\n",
    "hour_embedding = ...\n",
    "\n",
    "# Конкатенируем слой на эмбедингах\n",
    "all_embeddings = L.concatenate(...)\n",
    "\n",
    "# Полносвязные слой на эмбедингах:\n",
    "dense1 = L.Dense(10, activation='tanh')(...)\n",
    "\n",
    "# Конкатенируем c остальными признаками\n",
    "all_features = L.concatenate(...)\n",
    "\n",
    "# Еще полносвязные слой\n",
    "dense2 = L.Dense(6, activation='tanh')(...)\n",
    "\n",
    "# Выходной слой\n",
    "output = L.Dense(1, activation='linear')(...)\n",
    "\n",
    "# Формируем модель\n",
    "model = keras.Model(inputs=[cont_input, month_input, hour_input, weekday_input], \n",
    "                    outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae', 'mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = \\\n",
    "model.fit(x=[X_train_asis, X_train_month, X_train_hour, X_train_weekday],\n",
    "          y=y_train, batch_size=512, \n",
    "          epochs=500, \n",
    "          validation_data=[[X_valid_asis, X_valid_month, X_valid_hour, X_valid_weekday], y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics.history['loss'][5:], label='train_mae')\n",
    "plt.plot(metrics.history['val_loss'][5:], label='val_mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict([X_valid_asis, X_valid_month, X_valid_hour, X_valid_weekday])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.expm1(y_hat), np.expm1(y_valid))\n",
    "plt.plot(np.expm1(y_valid), np.expm1(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.layers[4]\n",
    "emb.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = emb.get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = linkage(weights, method='average', metric='cosine')\n",
    "d = dendrogram(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "155px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
